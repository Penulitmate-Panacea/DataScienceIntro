---
title: "Making Trees"
author: "Rian Fantozzi"
date: "10/17/2017"
output: pdf_document
---
Name(s):

**Data setup**

```{r setup, include=FALSE}
library(tidyverse)
library(mosaic)
library(rpart) #where the tree routines are
library(partykit) #for good tree visualization
library(sp)
library(randomForest)

```

We will be using data about patients with acute hepatitis and trying to classify which explanatory variables predict death of a patient.

Load the cleaned data.
```{r}
library(readr)
hep <- read_csv("C:/Users/rianf/Downloads/hepatitisCleanNoNA.csv")
```

Now you'll need to take all of the quantitative variables that are categorical and use as.factor like in the example from class to make R treat them like categories called factors (needed for randomForests).  
```{r}
hep=mutate(hep, Class=as.factor(Class),Sex=as.factor(Sex), Steriod=as.factor(Steriod), Antivirals = as.factor(Antivirals), Fatigue=as.factor(Fatigue),Malaise=as.factor(Malaise),Anorexia=as.factor(Anorexia),LiverBig=as.factor(LiverBig),LiverFirm=as.factor(LiverFirm),SpeelPalpable=as.factor(SpeelPalpable),Spiders=as.factor(Spiders),Acvites=as.factor(Acvites),Varacises=as.factor(Varacises))
head(hep)
```

Now let's pick a training set and a test set.
```{r}
set.seed(1337334651)
n=nrow(hep)
testSample=sample.int(n,size=round(.2*n))
train=hep[-testSample,]
test=hep[testSample,]

```

Our response variable will be Class. It has two values, 2=LIVE and 1=DIE.  Note for sex, 2-male and 1=female. For yes/no categorical variables 2=No and 1=yes.
```{r}
TreeOI=rpart(Class~., data=train, parms=list(split="information")) #the . tells R to use all the other variables
TreeOI
plot(as.party(TreeOI))
```

```{r}
forest=randomForest(Class~.,data=train, ntree=20000,mtry=4)
forest
```
```{r}
import=importance(forest) #this gives importance. The rest is getting to display in decreasing order
import
import=as.data.frame(import)
import=rownames_to_column(import)
import
arrange(import,desc(MeanDecreaseGini))

```

```{r}
predTest=predict(forest,test,type="class")
confFor=tally(predTest~ test$Class)
confFor
(sum(diag(confFor)))/nrow(test)
```

**Project directions**
Your job is to write a short paper talking about this data and trees made with it.

Begin with a description of the data set. You will need to look up the variables that show up in the paper to see what they mean so you can talk about them in context.

First, make a tree in R using all possible predcitors predicting Class with the Gini coefficent. For the tree you will show a picture of the tree, show how it will classify one case in the data set, and  evaluate the prediction ability on the test and training data using the confusion matrix. 

Second, make a tree with the information gain in R.

Third, make a tree in Weka (copy and paste the Weka text output into R).  The csv file will need a column name for the first column (call it Row), which is the one you can delete. You might convert most of the variables to nominal type (Filters -> Unsupervised -> attribute -> NumericToNominal and specify the column numbers) for nicer trees. You may then remove the Row number column that is explicit in the data.    Recall the decision tree algorithm is J48 under Classifiers -> Trees.

Test mode:    split 66.0% train, remainder test

=== Classifier model (full training set) ===


RandomTree
==========

Bilirubis < 1.65
|   Acvites = 1
|   |   Albumin < 2.85 : 1 (2/0)
|   |   Albumin >= 2.85
|   |   |   Sgot < 86
|   |   |   |   Bilirubis < 0.5 : 2 (1/0)
|   |   |   |   Bilirubis >= 0.5 : 1 (2/0)
|   |   |   Sgot >= 86 : 2 (3/0)
|   Acvites = 2
|   |   Bilirubis < 0.55
|   |   |   Sex = 1 : 1 (2/0)
|   |   |   Sex = 2 : 2 (2/0)
|   |   Bilirubis >= 0.55
|   |   |   Fatigue = 1
|   |   |   |   Spiders = 1
|   |   |   |   |   Bilirubis < 1.35
|   |   |   |   |   |   Albumin < 4.15 : 2 (9/0)
|   |   |   |   |   |   Albumin >= 4.15
|   |   |   |   |   |   |   Steriod = 1 : 2 (2/0)
|   |   |   |   |   |   |   Steriod = 2
|   |   |   |   |   |   |   |   Sgot < 46.5 : 1 (1/0)
|   |   |   |   |   |   |   |   Sgot >= 46.5 : 2 (1/0)
|   |   |   |   |   Bilirubis >= 1.35
|   |   |   |   |   |   Sgot < 62.5 : 2 (2/0)
|   |   |   |   |   |   Sgot >= 62.5 : 1 (2/0)
|   |   |   |   Spiders = 2 : 2 (33/0)
|   |   |   Fatigue = 2 : 2 (41/0)
Bilirubis >= 1.65
|   Sex = 1
|   |   Acvites = 1
|   |   |   Spiders = 1 : 1 (6/0)
|   |   |   Spiders = 2
|   |   |   |   Age < 42.5 : 2 (1/0)
|   |   |   |   Age >= 42.5 : 1 (2/0)
|   |   Acvites = 2
|   |   |   Fatigue = 1
|   |   |   |   Malaise = 1
|   |   |   |   |   Sgot < 211
|   |   |   |   |   |   Spiders = 1
|   |   |   |   |   |   |   SpeelPalpable = 1 : 1 (1/0)
|   |   |   |   |   |   |   SpeelPalpable = 2
|   |   |   |   |   |   |   |   Age < 50 : 2 (3/0)
|   |   |   |   |   |   |   |   Age >= 50 : 1 (1/0)
|   |   |   |   |   |   Spiders = 2 : 1 (2/0)
|   |   |   |   |   Sgot >= 211 : 2 (2/0)
|   |   |   |   Malaise = 2
|   |   |   |   |   SpeelPalpable = 1 : 1 (2/0)
|   |   |   |   |   SpeelPalpable = 2 : 2 (3/0)
|   |   |   Fatigue = 2 : 1 (1/0)
|   Sex = 2 : 2 (2/0)

Size of the tree : 51

Time taken to build model: 0.01 seconds

=== Evaluation on test split ===

Time taken to test model on test split: 0 seconds

=== Summary ===

Correctly Classified Instances          36               81.8182 %
Incorrectly Classified Instances         8               18.1818 %
Kappa statistic                          0.3973
Mean absolute error                      0.1818
Root mean squared error                  0.4264
Relative absolute error                 61.5929 %
Root relative squared error            120.8479 %
Total Number of Instances               44     

=== Detailed Accuracy By Class ===

                 TP Rate  FP Rate  Precision  Recall   F-Measure  MCC      ROC Area  PRC Area  Class
                 0.667    0.158    0.400      0.667    0.500      0.417    0.754     0.312     1
                 0.842    0.333    0.941      0.842    0.889      0.417    0.754     0.929     2
Weighted Avg.    0.818    0.309    0.867      0.818    0.836      0.417    0.754     0.845     

=== Confusion Matrix ===

  a  b   <-- classified as
  4  2 |  a = 1
  6 32 |  b = 2



Fourth, make a random forest in R. 

Finally, compare the first tree to the trees to those generated in the second, third and fourth steps. Your comparison will include what variables are new to the tree/importance list of the forest and the prediction ability on the trainig and test set.  

Conclude with what variables seem to be important to predicting Class and which tree you think should be used and why.
